{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFfzyfV0bJC9",
        "outputId": "9ec8f9c2-9d62-484c-844b-9cffdeb70f3c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "gIR0xkCXSvDS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import LxmertTokenizer, LxmertModel\n",
        "from torchvision import models\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the model architecture again as in your original code\n",
        "class ToxicMemeClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(ToxicMemeClassifier, self).__init__()\n",
        "        self.lxmert = LxmertModel.from_pretrained(\"unc-nlp/lxmert-base-uncased\")\n",
        "\n",
        "        # ResNet backbone\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "        self.cnn_backbone = nn.Sequential(*list(resnet.children())[:-1])  # Remove final FC layer\n",
        "        self.visual_fc = nn.Linear(resnet.fc.in_features, self.lxmert.config.visual_feat_dim)\n",
        "\n",
        "        # Classifier\n",
        "        self.fc = nn.Linear(self.lxmert.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, images):\n",
        "        visual_feats = self.cnn_backbone(images).squeeze(-1).squeeze(-1)\n",
        "        visual_feats = self.visual_fc(visual_feats).unsqueeze(1)\n",
        "\n",
        "        batch_size = visual_feats.size(0)\n",
        "        visual_pos = torch.zeros(batch_size, 1, 4).to(visual_feats.device)\n",
        "\n",
        "        outputs = self.lxmert(input_ids=input_ids, attention_mask=attention_mask, visual_feats=visual_feats, visual_pos=visual_pos)\n",
        "        logits = self.fc(outputs.pooled_output)\n",
        "        return logits\n",
        "\n",
        "# Initialize the model\n",
        "model = ToxicMemeClassifier(num_classes=2)\n",
        "\n",
        "# Load the model weights\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/toxic_meme_classifier.pth'))\n",
        "model = model.to(device)  # Move model to the right device (CPU or GPU)\n",
        "\n",
        "# If you want to switch the model to evaluation mode\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "WEoUd8uPSBqi",
        "outputId": "1ddab835-d8d6-446f-ba67-9f63d3c0ef16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at unc-nlp/lxmert-base-uncased were not used when initializing LxmertModel: ['answer_head.logit_fc.0.bias', 'answer_head.logit_fc.0.weight', 'answer_head.logit_fc.2.bias', 'answer_head.logit_fc.2.weight', 'answer_head.logit_fc.3.bias', 'answer_head.logit_fc.3.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'obj_predict_head.decoder_dict.attr.bias', 'obj_predict_head.decoder_dict.attr.weight', 'obj_predict_head.decoder_dict.feat.bias', 'obj_predict_head.decoder_dict.feat.weight', 'obj_predict_head.decoder_dict.obj.bias', 'obj_predict_head.decoder_dict.obj.weight', 'obj_predict_head.transform.LayerNorm.bias', 'obj_predict_head.transform.LayerNorm.weight', 'obj_predict_head.transform.dense.bias', 'obj_predict_head.transform.dense.weight']\n",
            "- This IS expected if you are initializing LxmertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LxmertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "<ipython-input-13-3c63ce855f7a>:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('/content/drive/MyDrive/toxic_meme_classifier.pth'))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ToxicMemeClassifier(\n",
              "  (lxmert): LxmertModel(\n",
              "    (embeddings): LxmertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768, padding_idx=0)\n",
              "      (token_type_embeddings): Embedding(2, 768, padding_idx=0)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): LxmertEncoder(\n",
              "      (visn_fc): LxmertVisualFeatureEncoder(\n",
              "        (visn_fc): Linear(in_features=2048, out_features=768, bias=True)\n",
              "        (visn_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (box_fc): Linear(in_features=4, out_features=768, bias=True)\n",
              "        (box_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (layer): ModuleList(\n",
              "        (0-8): 9 x LxmertLayer(\n",
              "          (attention): LxmertSelfAttentionLayer(\n",
              "            (self): LxmertAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): LxmertAttentionOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): LxmertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): LxmertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (x_layers): ModuleList(\n",
              "        (0-4): 5 x LxmertXLayer(\n",
              "          (visual_attention): LxmertCrossAttentionLayer(\n",
              "            (att): LxmertAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): LxmertAttentionOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (lang_self_att): LxmertSelfAttentionLayer(\n",
              "            (self): LxmertAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): LxmertAttentionOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (visn_self_att): LxmertSelfAttentionLayer(\n",
              "            (self): LxmertAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): LxmertAttentionOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (lang_inter): LxmertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (lang_output): LxmertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (visn_inter): LxmertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (visn_output): LxmertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (r_layers): ModuleList(\n",
              "        (0-4): 5 x LxmertLayer(\n",
              "          (attention): LxmertSelfAttentionLayer(\n",
              "            (self): LxmertAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): LxmertAttentionOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): LxmertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): LxmertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): LxmertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (cnn_backbone): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (4): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (5): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (6): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (4): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (5): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (7): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  )\n",
              "  (visual_fc): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "  (fc): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install easyocr"
      ],
      "metadata": {
        "collapsed": true,
        "id": "lbrGYJfZSUK8",
        "outputId": "9d82c101-1bfb-400e-cb17-21f4d7d94825",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting easyocr\n",
            "  Downloading easyocr-1.7.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from easyocr) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.11/dist-packages (from easyocr) (0.20.1+cu121)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (from easyocr) (4.11.0.86)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from easyocr) (1.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from easyocr) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from easyocr) (11.1.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from easyocr) (0.25.0)\n",
            "Collecting python-bidi (from easyocr)\n",
            "  Downloading python_bidi-0.6.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from easyocr) (6.0.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.11/dist-packages (from easyocr) (2.0.6)\n",
            "Collecting pyclipper (from easyocr)\n",
            "  Downloading pyclipper-1.3.0.post6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting ninja (from easyocr)\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->easyocr) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->easyocr) (1.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (2.36.1)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (2025.1.10)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->easyocr) (3.0.2)\n",
            "Downloading easyocr-1.7.2-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyclipper-1.3.0.post6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (969 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m969.6/969.6 kB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_bidi-0.6.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (286 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.6/286.6 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-bidi, pyclipper, ninja, easyocr\n",
            "Successfully installed easyocr-1.7.2 ninja-1.11.1.3 pyclipper-1.3.0.post6 python-bidi-0.6.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import LxmertTokenizer\n",
        "from torchvision import transforms\n",
        "import easyocr  # EasyOCR library for text extraction\n",
        "\n",
        "# Define the model architecture again as in your original code\n",
        "class ToxicMemeClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(ToxicMemeClassifier, self).__init__()\n",
        "        self.lxmert = LxmertModel.from_pretrained(\"unc-nlp/lxmert-base-uncased\")\n",
        "\n",
        "        # ResNet backbone\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "        self.cnn_backbone = nn.Sequential(*list(resnet.children())[:-1])  # Remove final FC layer\n",
        "        self.visual_fc = nn.Linear(resnet.fc.in_features, self.lxmert.config.visual_feat_dim)\n",
        "\n",
        "        # Classifier\n",
        "        self.fc = nn.Linear(self.lxmert.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, images):\n",
        "        visual_feats = self.cnn_backbone(images).squeeze(-1).squeeze(-1)\n",
        "        visual_feats = self.visual_fc(visual_feats).unsqueeze(1)\n",
        "\n",
        "        batch_size = visual_feats.size(0)\n",
        "        visual_pos = torch.zeros(batch_size, 1, 4).to(visual_feats.device)\n",
        "\n",
        "        outputs = self.lxmert(input_ids=input_ids, attention_mask=attention_mask, visual_feats=visual_feats, visual_pos=visual_pos)\n",
        "        logits = self.fc(outputs.pooled_output)\n",
        "        return logits\n",
        "\n",
        "# Function to load and preprocess a single image\n",
        "def preprocess_image(img_path, transform):\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "    image = transform(image)\n",
        "    return image.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Function to extract text from an image using EasyOCR\n",
        "def extract_text_from_image(img_path):\n",
        "    reader = easyocr.Reader(['en'])  # Specify the languages to recognize (English)\n",
        "    result = reader.readtext(img_path)\n",
        "\n",
        "    # Combine all the text extracted from the image\n",
        "    extracted_text = \" \".join([text[1] for text in result])  # `text[1]` contains the text part of the result\n",
        "    return extracted_text.strip()  # Remove any extra spaces/newlines\n",
        "\n",
        "# Function to tokenize text input\n",
        "def preprocess_text(text, tokenizer, max_len):\n",
        "    encoding = tokenizer(\n",
        "        text,\n",
        "        max_length=max_len,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    return encoding[\"input_ids\"].squeeze(0), encoding[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "# Define the paths for the test image\n",
        "image_path = \"/content/04615.png\"  # Replace with your image path\n",
        "\n",
        "# Load the model and weights\n",
        "model = ToxicMemeClassifier(num_classes=2)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/toxic_meme_classifier.pth'))  # Load your trained model\n",
        "model.to(device)  # Move the model to the appropriate device (CPU or GPU)\n",
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "# Extract text from the image using EasyOCR\n",
        "extracted_text = extract_text_from_image(image_path)\n",
        "print(f\"\\n\\n Extracted Text: {extracted_text}\")\n",
        "\n",
        "# Tokenizer and transform\n",
        "tokenizer = LxmertTokenizer.from_pretrained(\"unc-nlp/lxmert-base-uncased\")\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Preprocess the image and text\n",
        "image = preprocess_image(image_path, transform)\n",
        "input_ids, attention_mask = preprocess_text(extracted_text, tokenizer, max_len=128)\n",
        "\n",
        "# Move inputs to the appropriate device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "image = image.to(device)\n",
        "input_ids = input_ids.to(device)\n",
        "attention_mask = attention_mask.to(device)\n",
        "\n",
        "# Model inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids.unsqueeze(0), attention_mask.unsqueeze(0), image)\n",
        "    _, prediction = torch.max(outputs, dim=1)\n",
        "\n",
        "# Output the result\n",
        "prediction_label = prediction.item()\n",
        "print(f\"Prediction: {'Toxic' if prediction_label == 1 else 'Non-Toxic'}\")\n"
      ],
      "metadata": {
        "id": "vEFg6fMsT-Fo",
        "outputId": "0776e9e6-a0f9-4a53-d05b-3c0366e81470",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at unc-nlp/lxmert-base-uncased were not used when initializing LxmertModel: ['answer_head.logit_fc.0.bias', 'answer_head.logit_fc.0.weight', 'answer_head.logit_fc.2.bias', 'answer_head.logit_fc.2.weight', 'answer_head.logit_fc.3.bias', 'answer_head.logit_fc.3.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'obj_predict_head.decoder_dict.attr.bias', 'obj_predict_head.decoder_dict.attr.weight', 'obj_predict_head.decoder_dict.feat.bias', 'obj_predict_head.decoder_dict.feat.weight', 'obj_predict_head.decoder_dict.obj.bias', 'obj_predict_head.decoder_dict.obj.weight', 'obj_predict_head.transform.LayerNorm.bias', 'obj_predict_head.transform.LayerNorm.weight', 'obj_predict_head.transform.dense.bias', 'obj_predict_head.transform.dense.weight']\n",
            "- This IS expected if you are initializing LxmertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LxmertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "<ipython-input-11-e0552bfed986>:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('/content/drive/MyDrive/toxic_meme_classifier.pth'))  # Load your trained model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " Extracted Text: @ts the religion 0f peaces join exclusive isam! RESTAP\n",
            "Prediction: Toxic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import LxmertTokenizer, LxmertModel\n",
        "from torchvision import transforms, models\n",
        "import easyocr  # EasyOCR library for text extraction\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the model architecture again as in your original code\n",
        "class ToxicMemeClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(ToxicMemeClassifier, self).__init__()\n",
        "        self.lxmert = LxmertModel.from_pretrained(\"unc-nlp/lxmert-base-uncased\")\n",
        "\n",
        "        # ResNet backbone\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "        self.cnn_backbone = nn.Sequential(*list(resnet.children())[:-1])  # Remove final FC layer\n",
        "        self.visual_fc = nn.Linear(resnet.fc.in_features, self.lxmert.config.visual_feat_dim)\n",
        "\n",
        "        # Classifier\n",
        "        self.fc = nn.Linear(self.lxmert.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, images):\n",
        "        # Extract image features\n",
        "        visual_feats = self.cnn_backbone(images).squeeze(-1).squeeze(-1)\n",
        "        visual_feats = self.visual_fc(visual_feats).unsqueeze(1)\n",
        "\n",
        "        # Print image features for debugging\n",
        "        print(\"\\nVisual Features Extracted from Image: \", visual_feats)\n",
        "\n",
        "        batch_size = visual_feats.size(0)\n",
        "        visual_pos = torch.zeros(batch_size, 1, 4).to(visual_feats.device)\n",
        "\n",
        "        # Forward pass through LXMERT\n",
        "        outputs = self.lxmert(input_ids=input_ids, attention_mask=attention_mask, visual_feats=visual_feats, visual_pos=visual_pos)\n",
        "\n",
        "        # Print text features (LXMERT outputs)\n",
        "        print(\"\\nText Features Extracted from LXMERT: \", outputs.pooled_output)\n",
        "\n",
        "        logits = self.fc(outputs.pooled_output)\n",
        "        return logits\n",
        "\n",
        "# Function to load and preprocess a single image\n",
        "def preprocess_image(img_path, transform):\n",
        "    image = Image.open(img_path).convert(\"RGB\")\n",
        "    image = transform(image)\n",
        "    return image.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Function to extract text from an image using EasyOCR\n",
        "def extract_text_from_image(img_path):\n",
        "    reader = easyocr.Reader(['en'])  # Specify the languages to recognize (English)\n",
        "    result = reader.readtext(img_path)\n",
        "\n",
        "    # Combine all the text extracted from the image\n",
        "    extracted_text = \" \".join([text[1] for text in result])  # `text[1]` contains the text part of the result\n",
        "    return extracted_text.strip()  # Remove any extra spaces/newlines\n",
        "\n",
        "# Function to tokenize text input\n",
        "def preprocess_text(text, tokenizer, max_len):\n",
        "    encoding = tokenizer(\n",
        "        text,\n",
        "        max_length=max_len,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    return encoding[\"input_ids\"].squeeze(0), encoding[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "# Define the paths for the test image\n",
        "image_path = \"/content/04615.png\"  # Replace with your image path\n",
        "\n",
        "# Load the model and weights\n",
        "model = ToxicMemeClassifier(num_classes=2)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/toxic_meme_classifier.pth'))  # Load your trained model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Check for device\n",
        "model.to(device)  # Move the model to the appropriate device (CPU or GPU)\n",
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "# Extract text from the image using EasyOCR\n",
        "extracted_text = extract_text_from_image(image_path)\n",
        "print(f\"\\n\\nExtracted Text: {extracted_text}\")\n",
        "\n",
        "# Tokenizer and transform\n",
        "tokenizer = LxmertTokenizer.from_pretrained(\"unc-nlp/lxmert-base-uncased\")\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Preprocess the image and text\n",
        "image = preprocess_image(image_path, transform)\n",
        "input_ids, attention_mask = preprocess_text(extracted_text, tokenizer, max_len=128)\n",
        "\n",
        "# Move inputs to the appropriate device (CPU or GPU)\n",
        "image = image.to(device)\n",
        "input_ids = input_ids.to(device)\n",
        "attention_mask = attention_mask.to(device)\n",
        "\n",
        "# Model inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids.unsqueeze(0), attention_mask.unsqueeze(0), image)\n",
        "    _, prediction = torch.max(outputs, dim=1)\n",
        "\n",
        "# Output the result\n",
        "prediction_label = prediction.item()\n",
        "print(f\"Prediction: {'Toxic' if prediction_label == 1 else 'Non-Toxic'}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6HQWjZ6d4cW",
        "outputId": "0f6b2d2a-8cac-4fca-ed93-7aca99d19659"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at unc-nlp/lxmert-base-uncased were not used when initializing LxmertModel: ['answer_head.logit_fc.0.bias', 'answer_head.logit_fc.0.weight', 'answer_head.logit_fc.2.bias', 'answer_head.logit_fc.2.weight', 'answer_head.logit_fc.3.bias', 'answer_head.logit_fc.3.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'obj_predict_head.decoder_dict.attr.bias', 'obj_predict_head.decoder_dict.attr.weight', 'obj_predict_head.decoder_dict.feat.bias', 'obj_predict_head.decoder_dict.feat.weight', 'obj_predict_head.decoder_dict.obj.bias', 'obj_predict_head.decoder_dict.obj.weight', 'obj_predict_head.transform.LayerNorm.bias', 'obj_predict_head.transform.LayerNorm.weight', 'obj_predict_head.transform.dense.bias', 'obj_predict_head.transform.dense.weight']\n",
            "- This IS expected if you are initializing LxmertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LxmertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "<ipython-input-17-36fbb67bd73e>:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('/content/drive/MyDrive/toxic_meme_classifier.pth'))  # Load your trained model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Extracted Text: @ts the religion 0f peaces join exclusive isam! RESTAP\n",
            "\n",
            "Visual Features Extracted from Image:  tensor([[[ 0.5777, -0.2285, -0.0096,  ...,  0.1382,  0.2591,  0.3611]]],\n",
            "       device='cuda:0')\n",
            "\n",
            "Text Features Extracted from LXMERT:  tensor([[-1.6600e-01, -3.3041e-01,  4.1277e-01,  5.8318e-01, -3.2593e-01,\n",
            "         -1.7347e-02,  4.4208e-01, -3.7199e-01, -2.8956e-01, -8.0565e-01,\n",
            "         -8.3700e-01,  6.5749e-01,  2.0912e-01,  3.2769e-01,  5.1696e-01,\n",
            "          1.3359e-01, -8.2920e-02, -2.7219e-01, -8.5613e-01,  9.4024e-01,\n",
            "         -1.3720e-01, -2.9000e-01, -9.8220e-01,  4.6013e-01,  7.9458e-01,\n",
            "          2.9596e-02, -7.4779e-01, -7.3266e-01,  6.5154e-01, -1.9653e-01,\n",
            "          1.3376e-01,  6.7937e-01,  3.2790e-01,  5.3818e-01,  4.8988e-01,\n",
            "          9.3729e-01, -2.4095e-01, -4.1765e-01,  3.2539e-01,  7.5798e-01,\n",
            "         -5.6229e-01, -5.6466e-01,  9.6189e-01, -8.4199e-01,  3.7680e-02,\n",
            "         -1.7562e-01,  2.3624e-01, -9.9520e-01,  9.1602e-01, -3.5696e-01,\n",
            "          8.7288e-01,  1.5298e-01,  2.5043e-02, -3.0593e-01, -1.7547e-02,\n",
            "         -1.1869e-01,  5.5761e-01,  1.6129e-01, -5.4627e-02, -1.9646e-01,\n",
            "          8.5141e-01,  3.8306e-01,  9.9620e-01,  1.6098e-01, -1.7496e-01,\n",
            "          7.9984e-01, -6.1024e-01, -5.3987e-01, -6.8238e-01,  5.1237e-01,\n",
            "         -2.5826e-01,  6.3177e-01,  1.1477e-01,  1.6337e-01, -8.9391e-01,\n",
            "          7.1075e-01, -7.1984e-01,  4.9460e-01, -9.8828e-01,  8.9147e-01,\n",
            "         -5.1386e-01, -9.8350e-01, -9.6147e-01, -7.2588e-01, -9.0572e-01,\n",
            "         -2.7468e-01, -8.1597e-01, -9.3116e-01,  7.9176e-01, -7.1787e-03,\n",
            "         -9.2705e-02,  2.0509e-01,  1.5262e-01,  2.0721e-02,  2.3233e-02,\n",
            "         -8.7323e-01, -5.4552e-01, -3.3218e-01, -3.2661e-01, -6.9566e-01,\n",
            "         -6.3498e-01, -2.8902e-01, -4.9436e-01,  8.9322e-01, -7.4496e-01,\n",
            "          5.4055e-01,  6.9712e-02,  4.8491e-01, -7.9719e-02,  9.6467e-01,\n",
            "          3.6487e-01, -4.2098e-01,  9.9786e-01,  3.0249e-01, -5.1290e-01,\n",
            "         -7.9535e-01, -5.6737e-01,  4.8832e-03,  9.5022e-01, -5.1951e-01,\n",
            "         -6.9659e-01,  1.1098e-01, -5.4661e-01, -1.9407e-01,  9.0242e-01,\n",
            "          8.6225e-01, -4.5983e-01, -6.9937e-01,  9.4356e-01,  1.0859e-01,\n",
            "         -9.7571e-01,  9.9908e-01, -5.3040e-01,  3.8831e-01, -8.2962e-01,\n",
            "          9.6730e-01,  9.1723e-02,  9.9889e-01,  9.4823e-01, -7.0785e-01,\n",
            "          2.4823e-01,  3.5889e-01, -1.4327e-01,  2.6735e-02, -3.3045e-01,\n",
            "         -9.0810e-01, -5.4554e-01, -7.1415e-05, -8.8249e-02, -8.1890e-01,\n",
            "         -4.8557e-01,  6.3062e-01,  5.9532e-01, -7.8344e-01,  8.5886e-01,\n",
            "          2.7650e-01,  7.9598e-01, -7.9093e-01,  3.6790e-01,  7.0413e-01,\n",
            "          4.1009e-01, -8.7822e-02, -4.9040e-01,  4.2084e-01, -4.0753e-01,\n",
            "          4.8854e-01, -4.3122e-01, -1.7872e-01,  5.7616e-01, -2.9799e-01,\n",
            "         -6.6575e-01, -7.1685e-01,  4.8931e-01,  4.1576e-01,  4.2848e-02,\n",
            "         -8.7281e-01, -1.7518e-01, -9.9483e-01, -5.7858e-02, -6.8668e-02,\n",
            "          4.1219e-01, -6.9640e-02,  3.3036e-02,  7.2169e-01, -3.0402e-01,\n",
            "         -6.6585e-01, -4.1859e-01,  9.6876e-01,  2.0388e-02, -9.5196e-02,\n",
            "          8.9773e-03,  5.5850e-01,  1.4917e-01,  9.1116e-01,  1.3642e-01,\n",
            "          4.1044e-01,  8.1133e-01,  1.1724e-01,  5.1986e-01, -7.5921e-01,\n",
            "          3.3034e-01,  8.3304e-02, -7.1799e-01, -8.2256e-01,  6.7231e-01,\n",
            "          2.8536e-01,  6.1660e-01,  4.6992e-01,  6.6498e-01,  3.2191e-01,\n",
            "         -2.2816e-01, -2.7081e-01,  9.1720e-01, -4.6257e-02,  5.3637e-01,\n",
            "         -8.0156e-01, -1.4660e-01,  9.3632e-01,  3.7514e-01,  4.5493e-01,\n",
            "          2.9023e-01, -5.8352e-01,  2.4914e-01,  3.3893e-03, -6.2953e-01,\n",
            "         -1.2596e-01, -5.1738e-02,  9.4231e-01,  6.3804e-01, -1.9080e-02,\n",
            "          3.7612e-01, -8.6356e-01,  8.8170e-01, -8.6632e-02, -1.2007e-01,\n",
            "         -6.5299e-01,  6.8148e-01, -4.0780e-01, -2.3593e-01,  4.2660e-01,\n",
            "          7.8375e-01, -4.1526e-01, -2.4750e-02, -1.8888e-01,  1.2846e-02,\n",
            "          8.0808e-01, -9.4930e-01,  3.8087e-02, -1.4915e-01, -6.6823e-01,\n",
            "         -5.7838e-01, -2.5024e-02, -5.0841e-01,  6.3996e-02,  1.2889e-01,\n",
            "         -3.3792e-01,  3.8474e-01, -2.9281e-01, -7.4345e-01, -4.5412e-01,\n",
            "         -7.3019e-01, -7.0542e-02,  5.8356e-02, -7.9789e-01,  5.3270e-01,\n",
            "         -2.4758e-01,  1.0572e-02, -4.4738e-01,  5.9936e-01,  7.6665e-01,\n",
            "          1.4394e-01,  3.5130e-01,  5.6884e-02,  4.5027e-01,  4.7846e-01,\n",
            "          8.6531e-01,  1.4587e-01,  7.0262e-01, -2.4995e-01, -3.0317e-01,\n",
            "         -7.1561e-01,  5.5785e-01, -6.6887e-01, -3.0305e-02, -4.2749e-01,\n",
            "         -3.3467e-01,  8.6056e-01, -4.0178e-01,  1.8228e-01, -6.5349e-01,\n",
            "         -7.3247e-02, -5.6426e-01,  2.5555e-01,  5.0985e-01, -9.5512e-01,\n",
            "         -2.7415e-01,  9.1716e-02,  8.4316e-01, -1.3996e-01,  3.3356e-01,\n",
            "          3.4345e-01, -6.5147e-01, -1.8612e-01,  7.9742e-01, -9.1954e-01,\n",
            "          3.2726e-01,  2.9059e-01, -6.7195e-01,  2.5843e-01,  1.6799e-03,\n",
            "          6.8190e-01,  2.8122e-01, -8.3350e-01,  3.8485e-01, -5.3598e-01,\n",
            "         -8.0532e-01,  4.2617e-01, -2.6679e-01,  9.1937e-01, -1.4495e-01,\n",
            "         -2.0258e-01, -2.6732e-02, -6.6505e-01, -9.4837e-01,  4.6576e-02,\n",
            "         -9.4101e-01,  6.1344e-01,  5.0610e-01,  3.2138e-01,  3.1310e-01,\n",
            "          3.5054e-01,  8.9426e-01,  4.6211e-01,  4.2963e-02,  2.5813e-02,\n",
            "         -4.7871e-01,  9.9078e-02,  3.9681e-01, -7.1772e-01, -7.9096e-01,\n",
            "         -4.3744e-02,  1.9644e-01, -1.9146e-02,  1.0498e-01, -5.8488e-01,\n",
            "          5.3173e-01,  2.3536e-01,  1.3192e-01, -8.8794e-01,  8.7464e-01,\n",
            "          7.4812e-01,  8.4845e-01, -9.1405e-01,  1.0657e-01,  4.8282e-01,\n",
            "          3.1054e-01,  7.2287e-01,  2.9666e-01, -4.6299e-01,  6.2479e-01,\n",
            "         -6.9773e-01, -8.8565e-01,  1.0781e-01,  1.7483e-01, -7.8926e-01,\n",
            "          2.1542e-01,  5.5839e-01,  9.7717e-01,  8.8704e-01, -1.8290e-01,\n",
            "          4.5365e-01,  5.1875e-01,  7.5776e-01, -4.4705e-01, -5.9369e-01,\n",
            "         -6.2702e-01, -5.7801e-01,  3.8057e-01,  9.0928e-02,  3.8826e-01,\n",
            "         -1.5107e-01, -4.1644e-01,  2.2576e-01, -3.7233e-01, -7.9008e-01,\n",
            "         -6.4463e-01, -5.9604e-01,  5.5097e-01,  3.4382e-02, -4.9155e-01,\n",
            "          5.5478e-01,  8.8212e-01, -5.8259e-01, -1.7745e-02,  8.5394e-01,\n",
            "          5.7481e-01, -1.8163e-01,  5.0683e-01, -7.3765e-01,  4.4554e-01,\n",
            "          8.2943e-01,  1.2047e-02, -3.0370e-01, -7.5166e-02,  6.0119e-01,\n",
            "         -7.1850e-01, -8.3111e-01,  9.1525e-01, -4.3894e-01,  8.9637e-01,\n",
            "          4.5673e-01,  3.8012e-01,  5.3573e-01, -2.2466e-01, -9.4269e-01,\n",
            "          4.9696e-01,  2.7213e-01, -9.1846e-01,  3.5052e-01,  4.8651e-01,\n",
            "         -5.4629e-01,  4.7083e-01,  1.9124e-01,  2.1066e-01,  7.1952e-02,\n",
            "          2.1996e-01,  5.3568e-01,  5.3860e-01, -2.7812e-01,  8.7598e-03,\n",
            "          8.4599e-01, -6.0924e-01, -2.0889e-01, -7.0826e-01,  3.5952e-01,\n",
            "         -6.5081e-01,  8.7461e-02, -3.9355e-01, -3.3568e-02,  5.3387e-01,\n",
            "          9.9335e-01, -8.2323e-01, -1.7681e-01,  1.0023e-01, -3.7389e-01,\n",
            "          4.1759e-01,  6.5251e-01, -7.0476e-01,  1.6354e-01,  7.1581e-04,\n",
            "         -3.0731e-01,  5.9588e-01,  7.2803e-01, -8.3505e-01, -2.8552e-01,\n",
            "          5.7176e-01,  2.4092e-01,  8.6470e-01, -2.7233e-01, -5.8119e-01,\n",
            "          4.0157e-01, -2.3377e-01,  3.3402e-01, -6.6281e-01, -2.6908e-01,\n",
            "         -9.1698e-01,  7.6743e-01,  7.1264e-01, -6.9476e-01,  9.3690e-01,\n",
            "         -3.6543e-01,  5.1705e-01,  7.2062e-03, -8.0607e-01, -4.9426e-01,\n",
            "         -5.7831e-01,  3.6232e-01,  1.9281e-01,  8.3132e-01,  3.8848e-01,\n",
            "         -5.9558e-01,  3.5720e-01,  5.5307e-01,  3.4347e-01,  3.9927e-01,\n",
            "         -1.3082e-01,  2.1942e-02,  5.0967e-01, -9.7069e-01, -1.6662e-01,\n",
            "          4.5614e-01,  9.9249e-01, -4.2398e-01, -4.0301e-01,  1.5048e-01,\n",
            "          8.9648e-01, -1.4432e-01,  3.0438e-01,  7.0956e-03,  6.8414e-01,\n",
            "          5.8771e-01, -5.2184e-01, -4.5917e-01,  1.6486e-01,  5.8309e-01,\n",
            "         -7.0341e-01, -6.3982e-01,  4.4675e-01,  4.3613e-01,  3.0230e-01,\n",
            "          7.8998e-01, -7.9304e-01,  4.1980e-01,  4.5979e-02, -5.7280e-01,\n",
            "         -7.6527e-01,  2.7476e-01, -8.4681e-01,  6.6760e-01,  3.2317e-01,\n",
            "         -9.2360e-01, -3.1524e-01, -5.8529e-01,  3.9595e-01, -9.9909e-01,\n",
            "         -2.5756e-01,  3.7210e-01,  1.6937e-01, -2.2155e-01,  7.6196e-01,\n",
            "         -5.4551e-01, -6.3642e-01, -7.7078e-01,  2.5631e-01,  4.3973e-01,\n",
            "          1.8968e-01, -8.4669e-01,  3.4869e-01, -5.8254e-01,  3.4997e-01,\n",
            "         -4.2545e-01,  3.4248e-01,  5.2377e-01,  6.8862e-02, -7.9806e-01,\n",
            "          9.6049e-01, -1.6712e-01, -4.5783e-01,  4.8202e-01,  3.6471e-01,\n",
            "         -9.1722e-01, -2.2693e-01,  9.4396e-01,  9.1733e-01, -2.5817e-01,\n",
            "         -1.7224e-01,  5.1857e-01,  5.1229e-01,  8.8673e-01,  4.9735e-01,\n",
            "         -9.0654e-01, -2.5349e-01, -2.9669e-02,  1.9466e-02, -8.5894e-01,\n",
            "          8.8992e-02,  6.0694e-01,  7.8397e-01, -4.8823e-01, -7.7745e-01,\n",
            "          7.5595e-01, -3.3466e-01, -9.4193e-01, -3.9703e-01, -1.8793e-02,\n",
            "          4.6454e-02,  3.8706e-01, -9.0874e-01, -6.7497e-01, -8.0356e-01,\n",
            "          8.5414e-01,  5.7330e-01, -9.9067e-01, -1.1578e-01,  5.0994e-01,\n",
            "         -8.1786e-01, -1.6883e-01,  5.2310e-01,  2.6423e-01, -3.0146e-01,\n",
            "         -3.7516e-02,  9.9288e-01, -8.8044e-02, -9.6518e-01,  7.7785e-01,\n",
            "         -5.5425e-02, -7.9491e-01,  3.8519e-01,  7.3331e-01,  7.7544e-01,\n",
            "          1.5572e-01,  5.7061e-02,  6.1080e-01,  7.6815e-01, -7.1719e-01,\n",
            "         -1.5284e-01,  8.8771e-01, -3.4006e-01,  5.1347e-01,  2.3626e-01,\n",
            "          9.2194e-01,  5.2877e-01,  5.4868e-01, -7.9429e-02, -2.1356e-01,\n",
            "          9.9850e-01,  3.0310e-01, -5.6244e-01, -5.4917e-01, -6.7593e-01,\n",
            "         -3.6805e-01, -2.1719e-01,  1.2208e-01,  5.6689e-01,  1.4933e-01,\n",
            "         -5.2568e-01,  1.7248e-01, -6.2875e-01, -7.8066e-01, -7.7640e-01,\n",
            "         -5.1435e-03, -2.0455e-01, -3.1870e-01, -8.4853e-01,  9.8425e-01,\n",
            "         -8.9933e-01, -7.4963e-01,  2.8828e-01,  4.2721e-01,  9.4503e-01,\n",
            "         -1.0634e-01,  9.9475e-01,  1.8469e-01,  1.9697e-01,  2.9559e-01,\n",
            "         -9.5590e-01, -3.2447e-01,  4.0197e-01,  2.3940e-01, -4.7721e-01,\n",
            "         -4.3271e-01,  9.5913e-01, -5.9297e-01, -1.5934e-01,  6.5353e-01,\n",
            "         -2.7330e-02, -4.1268e-01,  8.6333e-01,  4.2616e-01,  7.0867e-01,\n",
            "         -5.2102e-01,  2.4131e-01, -2.2255e-01,  4.1607e-02,  7.5627e-02,\n",
            "         -6.2632e-01,  2.3437e-01,  5.2047e-01,  3.0257e-01,  1.3103e-01,\n",
            "          1.8911e-01, -9.0993e-01,  5.3299e-01,  7.2894e-01,  4.0728e-01,\n",
            "         -5.1355e-01, -9.8777e-02, -2.3848e-01, -3.1406e-02, -3.3111e-01,\n",
            "          5.8738e-01,  5.5292e-02,  3.0022e-01,  5.6762e-01,  2.3356e-01,\n",
            "          7.1069e-01,  7.2894e-01, -5.5466e-01,  4.1524e-01, -9.7361e-01,\n",
            "          6.6617e-01, -9.8307e-01, -3.2706e-01, -3.2111e-01,  5.3643e-02,\n",
            "         -1.8737e-01, -9.6479e-01,  9.8782e-01,  6.7510e-01,  1.2381e-01,\n",
            "         -6.6350e-01,  3.6554e-01,  5.7567e-01, -6.5666e-01, -1.5908e-02,\n",
            "         -7.3924e-01, -3.6660e-01,  2.6709e-01, -8.9048e-01,  8.9415e-02,\n",
            "          6.3008e-01,  6.1950e-01,  3.9096e-01,  3.1018e-01,  2.6322e-01,\n",
            "          5.2448e-01, -5.2457e-01,  6.8555e-01,  7.4819e-01,  3.8392e-01,\n",
            "         -8.6323e-01, -7.2732e-01, -2.5227e-01, -6.3381e-01, -1.4964e-01,\n",
            "         -6.5254e-01,  6.7548e-01,  1.6848e-01,  1.0635e-01, -1.6095e-01,\n",
            "         -2.3554e-01, -5.6298e-01,  4.0444e-01,  4.2508e-01, -9.4753e-01,\n",
            "         -4.6060e-01,  4.5142e-01,  2.3755e-01, -4.9269e-01,  4.3763e-02,\n",
            "          9.1919e-01, -2.5125e-01, -6.9145e-01,  4.3161e-01, -5.5145e-02,\n",
            "         -9.6859e-01,  2.2888e-01, -5.9566e-02,  5.4752e-01, -4.8457e-03,\n",
            "         -2.3264e-01, -2.4724e-03,  2.2338e-01, -3.1608e-01,  7.5816e-01,\n",
            "          6.2703e-01,  9.5696e-01, -9.1129e-01, -2.1834e-01,  5.7002e-02,\n",
            "          6.3983e-01, -6.1858e-01,  6.4357e-01, -4.1622e-01, -8.4612e-01,\n",
            "         -9.3432e-02,  8.9558e-01,  2.1301e-01]], device='cuda:0')\n",
            "Prediction: Toxic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ewK9aRcufec4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}