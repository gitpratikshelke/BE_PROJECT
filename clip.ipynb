{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "873b0289-5ca8-4285-bdbf-9268817796a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Load the CLIP processor and model\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73a7f1ab-bce4-409c-b771-fd34172e2e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_toxicity(text):\n",
    "    # Define the label texts (toxic, non-toxic)\n",
    "    labels = [\"toxic\", \"non-toxic\"]\n",
    "    \n",
    "    # Preprocess the text (text input for CLIP)\n",
    "    inputs = processor(text=[text] * len(labels), text_pair=labels, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Get the text features\n",
    "    outputs = model(**inputs)\n",
    "    text_features = outputs.text_embeds  # Get the text embeddings from the model\n",
    "    \n",
    "    # Calculate cosine similarities between the input text and each label\n",
    "    similarity_scores = torch.nn.functional.cosine_similarity(text_features[0], outputs.text_embeds[1:])\n",
    "    \n",
    "    # Get the label with the highest similarity score\n",
    "    predicted_label = labels[similarity_scores.argmax().item()]\n",
    "    toxicity_score = similarity_scores.max().item()\n",
    "    \n",
    "    return predicted_label, toxicity_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9edd5f97-20b1-4248-a5f5-adc7a156fc0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You have to specify pixel_values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m label, score\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Apply the function to the dataset\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoxicity_label\u001b[39m\u001b[38;5;124m\"\u001b[39m], data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoxicity_score\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mdata\u001b[38;5;241m.\u001b[39mapply(apply_clip_model, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# View the results\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\pandas\\core\\frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m  10373\u001b[0m )\n\u001b[1;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mapply()\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\pandas\\core\\apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[1;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\pandas\\core\\apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_generator()\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\pandas\\core\\apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(v, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m, in \u001b[0;36mapply_clip_model\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_clip_model\u001b[39m(row):\n\u001b[0;32m     13\u001b[0m     text \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtractedText\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 14\u001b[0m     label, score \u001b[38;5;241m=\u001b[39m classify_toxicity(text)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m label, score\n",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m, in \u001b[0;36mclassify_toxicity\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      6\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(text\u001b[38;5;241m=\u001b[39m[text] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels), text_pair\u001b[38;5;241m=\u001b[39mlabels, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Get the text features\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m     10\u001b[0m text_features \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mtext_embeds  \u001b[38;5;66;03m# Get the text embeddings from the model\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Calculate cosine similarities between the input text and each label\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:1362\u001b[0m, in \u001b[0;36mCLIPModel.forward\u001b[1;34m(self, input_ids, pixel_values, attention_mask, position_ids, return_loss, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[0;32m   1357\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1358\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[0;32m   1359\u001b[0m )\n\u001b[0;32m   1360\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1362\u001b[0m vision_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_model(\n\u001b[0;32m   1363\u001b[0m     pixel_values\u001b[38;5;241m=\u001b[39mpixel_values,\n\u001b[0;32m   1364\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1365\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1366\u001b[0m     interpolate_pos_encoding\u001b[38;5;241m=\u001b[39minterpolate_pos_encoding,\n\u001b[0;32m   1367\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1368\u001b[0m )\n\u001b[0;32m   1370\u001b[0m text_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_model(\n\u001b[0;32m   1371\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1372\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1376\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1377\u001b[0m )\n\u001b[0;32m   1379\u001b[0m image_embeds \u001b[38;5;241m=\u001b[39m vision_outputs[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mD:\\Anaconda\\Lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:1091\u001b[0m, in \u001b[0;36mCLIPVisionTransformer.forward\u001b[1;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict, interpolate_pos_encoding)\u001b[0m\n\u001b[0;32m   1088\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1091\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify pixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1093\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(pixel_values, interpolate_pos_encoding\u001b[38;5;241m=\u001b[39minterpolate_pos_encoding)\n\u001b[0;32m   1094\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_layrnorm(hidden_states)\n",
      "\u001b[1;31mValueError\u001b[0m: You have to specify pixel_values"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your CSV data\n",
    "file_path = \"E:\\\\BE Project\\\\archive (3)\\\\MIMIC2024 ee.csv\"\n",
    "data = pd.read_csv(file_path, encoding=\"latin1\")\n",
    "\n",
    "# Handle missing values\n",
    "data = data.dropna(subset=[\"ExtractedText\"])\n",
    "data[\"ExtractedText\"] = data[\"ExtractedText\"].fillna(\"\")\n",
    "\n",
    "# Define a function to apply the model\n",
    "def apply_clip_model(row):\n",
    "    text = row[\"ExtractedText\"]\n",
    "    label, score = classify_toxicity(text)\n",
    "    return label, score\n",
    "\n",
    "# Apply the function to the dataset\n",
    "data[\"toxicity_label\"], data[\"toxicity_score\"] = zip(*data.apply(apply_clip_model, axis=1))\n",
    "\n",
    "# View the results\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4801996a-f10f-48a0-97b1-b00a450e691a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a00796a14094a8d8e0595cd68e3ee27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\SAGAR_007\\.cache\\huggingface\\hub\\models--openai--clip-vit-base-patch16. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e589cbbdf4c44c79928f492d815218a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/599M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d0e09940524877a0debd7253e2a5e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc88df5f37ef4a489851868b28dbe448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecdccff672fd4447b6bb90f962ac2317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/961k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa5b678870a43ecb2560f5d29de4cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/599M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "210f47a659544e0185d2178a4f3c5200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62b19ed661d47308dccd2f8d13451cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18870bdc096e454499e04d4fe8d7076d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            FileName                                      ExtractedText  \\\n",
      "0   861_M_pic_12.jpg                    haldi wala doodh turmeric latte   \n",
      "1  2171_M_pic_12.jpg                                à¥­OKA BOLO QDOUDDO   \n",
      "2     313_NM_pic.jpg  Kuch Mahino baad ye bhi Maa ban jayegi! Taimur...   \n",
      "3   1458_M_pic_1.jpg                          Mujhe to kuch aur hi laga   \n",
      "4    2018_NM_pic.jpg  Teacher tum kal school kyu nhi aaye the Me kyu...   \n",
      "\n",
      "  toxicity_label  toxicity_score  \n",
      "0          toxic        0.548819  \n",
      "1      non-toxic        0.779532  \n",
      "2      non-toxic        0.650416  \n",
      "3      non-toxic        0.769572  \n",
      "4      non-toxic        0.775010  \n",
      "Results saved to E:\\BE Project\\processed_clip_toxicity_scores.csv\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from transformers import CLIPProcessor, CLIPModel\n",
    "# import pandas as pd\n",
    "\n",
    "# # Load the CLIP processor and model\n",
    "# from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Load pre-trained model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "def classify_toxicity(text):\n",
    "    # Define the labels for toxicity classification\n",
    "    labels = [\"toxic\", \"non-toxic\"]\n",
    "    \n",
    "    # Preprocess the text and truncate if necessary\n",
    "    inputs = processor(text=[text] * len(labels), text_pair=labels, return_tensors=\"pt\", padding=True, truncation=True, max_length=77)\n",
    "    \n",
    "    # Get the text features from the model\n",
    "    outputs = model.get_text_features(**inputs)\n",
    "    \n",
    "    # Preprocess the label texts\n",
    "    label_inputs = processor(text=labels, return_tensors=\"pt\", padding=True, truncation=True, max_length=77)\n",
    "    \n",
    "    # Get the label features\n",
    "    label_outputs = model.get_text_features(**label_inputs)\n",
    "    \n",
    "    # Calculate cosine similarities between the input text and each label\n",
    "    cosine_similarities = torch.nn.functional.cosine_similarity(outputs, label_outputs)\n",
    "    \n",
    "    # Get the label with the highest similarity score\n",
    "    max_similarity_idx = torch.argmax(cosine_similarities).item()\n",
    "    label = labels[max_similarity_idx]\n",
    "    score = cosine_similarities[max_similarity_idx].item()\n",
    "    \n",
    "    return label, score\n",
    "\n",
    "# Apply the function to the dataset\n",
    "data[\"toxicity_label\"], data[\"toxicity_score\"] = zip(*data.apply(apply_clip_model, axis=1))\n",
    "\n",
    "# View the results\n",
    "print(data.head())\n",
    "\n",
    "\n",
    "# Save the results to a CSV\n",
    "output_path = \"E:\\\\BE Project\\\\processed_clip_toxicity_scores.csv\"\n",
    "data.to_csv(output_path, index=False)\n",
    "print(f\"Results saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "948a869d-481d-4b3d-866a-b69bf9a43d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to E:\\BE Project\\processed_clip_toxicity_scores.csv\n"
     ]
    }
   ],
   "source": [
    "output_path = \"E:\\\\BE Project\\\\processed_clip_toxicity_scores.csv\"\n",
    "data.to_csv(output_path, index=False)\n",
    "print(f\"Results saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b673808-3ee6-42c0-8fde-ac845b4710c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          ExtractedText  toxicity_score\n",
      "1855                                      mind to Me My        0.875959\n",
      "3374                                           0 YAIS H        0.863691\n",
      "3528                                         Imemesl Ia        0.863364\n",
      "3801                                              POLIC        0.863350\n",
      "1166                                                  ^        0.862748\n",
      "...                                                 ...             ...\n",
      "4768  Bajrang Dal worker beats himself with a stick ...        0.462637\n",
      "3423  HANSIKA MOTANI IN KOI MIL GAYA AAPKA SUROOR (2...        0.424970\n",
      "451   Hello Mahi Bhai Hello Cheeku Bhai HEHE GEVTRAL...        0.423248\n",
      "1758       Hollywood Ka Pankaj Tripathi @conlused.aatma        0.413148\n",
      "2177  ARYAN KHAN ON DINNER TABLE AT MANNAT Papa hum ...        0.405317\n",
      "\n",
      "[1303 rows x 2 columns]\n",
      "Number of toxic comments: 3993\n"
     ]
    }
   ],
   "source": [
    "# View the top toxic comments\n",
    "top_toxic = data[data[\"toxicity_label\"] == \"toxic\"].sort_values(by=\"toxicity_score\", ascending=False)\n",
    "print(top_toxic[[\"ExtractedText\", \"toxicity_score\"]])\n",
    "\n",
    "# Filter based on a toxicity threshold\n",
    "threshold = 0.7  # Define a toxicity threshold\n",
    "toxic_comments = data[data[\"toxicity_score\"] > threshold]\n",
    "print(f\"Number of toxic comments: {len(toxic_comments)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2755ae3-d6ef-4ae6-8049-ddae286bf751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:12: SyntaxWarning: invalid escape sequence '\\B'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\B'\n",
      "C:\\Users\\SAGAR_007\\AppData\\Local\\Temp\\ipykernel_9836\\3996921128.py:12: SyntaxWarning: invalid escape sequence '\\B'\n",
      "  meme_directory = \"E:\\BE Project\\archive (3)\\Files\"\n",
      "C:\\Users\\SAGAR_007\\AppData\\Local\\Temp\\ipykernel_9836\\3996921128.py:12: SyntaxWarning: invalid escape sequence '\\B'\n",
      "  meme_directory = \"E:\\BE Project\\archive (3)\\Files\"\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[WinError 123] The filename, directory name, or volume label syntax is incorrect: 'E:\\\\BE Project\\x07rchive (3)\\\\Files'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Process all meme images in the directory\u001b[39;00m\n\u001b[0;32m     41\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(meme_directory):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     44\u001b[0m         image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(meme_directory, filename)\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 123] The filename, directory name, or volume label syntax is incorrect: 'E:\\\\BE Project\\x07rchive (3)\\\\Files'"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from transformers import CLIPProcessor, CLIPModel\n",
    "# from PIL import Image\n",
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# Initialize the CLIP model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Define the meme image directory\n",
    "meme_directory = \"E:/BE Project/archive (3)/memes\"\n",
    "\n",
    "# Example function to classify toxicity based on image and text\n",
    "def classify_toxicity(image_path, text):\n",
    "    # Open and preprocess the image\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Prepare the inputs (image and text)\n",
    "    inputs = processor(text=[text], images=image, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Get the outputs from the model\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # Extract the image and text features\n",
    "    image_features = outputs.image_embeds\n",
    "    text_features = outputs.text_embeds\n",
    "    \n",
    "    # Calculate the similarity score (cosine similarity)\n",
    "    similarity = torch.cosine_similarity(image_features, text_features)\n",
    "    \n",
    "    # Define a threshold for toxicity (example threshold)\n",
    "    toxicity_threshold = 0.7\n",
    "    \n",
    "    # If the similarity score is below the threshold, classify as toxic\n",
    "    label = \"Toxic\" if similarity < toxicity_threshold else \"Non-toxic\"\n",
    "    \n",
    "    return label, similarity.item()\n",
    "\n",
    "# Process all meme images in the directory\n",
    "results = []\n",
    "for filename in os.listdir(meme_directory):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "        image_path = os.path.join(meme_directory, filename)\n",
    "        text = \"some text from the meme caption\"  # Replace this with the actual text from the meme\n",
    "        \n",
    "        label, score = classify_toxicity(image_path, text)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"image\": filename,\n",
    "            \"toxicity_label\": label,\n",
    "            \"toxicity_score\": score\n",
    "        })\n",
    "\n",
    "# Convert results into a DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save results to a CSV file\n",
    "df.to_csv(\"E:\\\\BE Project\\\\meme_toxicity_results.csv\", index=False)\n",
    "\n",
    "# You can also print the DataFrame if needed\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e4cbb50-b76a-4c90-b4e4-2a440f3718a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SAGAR_007\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11201a58-08f0-4bd6-8793-70054ca807cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytesseract in d:\\anaconda\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: packaging>=21.3 in d:\\anaconda\\lib\\site-packages (from pytesseract) (23.2)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in d:\\anaconda\\lib\\site-packages (from pytesseract) (10.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "22775a9a-c7cc-471e-95c2-6da92336dc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              image                                     extracted_text  \\\n",
      "0   100_M_pic_1.jpg  Abba me ane! hona chahta hu; b!\\n\"jail meygahu...   \n",
      "1  101_M_pic_10.jpg                     One Day with my two wife “@:@3   \n",
      "2    101_NM_pic.jpg  ©\\n>\\n)\\nis\\nGS\\n2\\na}\\n£\\ni)\\n=\\n@\\n))\\n=\\nc=...   \n",
      "3    102_NM_pic.jpg  Cu,\\nbe x oe |)\\nDUNIYA HILA DENGE HUM.\\nIG:@C...   \n",
      "4    103_NM_pic.jpg  Bachapanika dard\\n\\nBhut — dard — .\\n\\nJb ag k...   \n",
      "\n",
      "  toxicity_label  toxicity_score  \n",
      "0          Toxic        0.234632  \n",
      "1          Toxic        0.261696  \n",
      "2          Toxic        0.220505  \n",
      "3          Toxic        0.291710  \n",
      "4          Toxic        0.302451  \n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from transformers import CLIPProcessor, CLIPModel\n",
    "# from PIL import Image\n",
    "# import os\n",
    "# import pandas as pd\n",
    "import pytesseract\n",
    "\n",
    "# Path to Tesseract executable (make sure it's correctly set for your system)\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'  # Change the path accordingly\n",
    "\n",
    "# Initialize the CLIP model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Define the meme image directory\n",
    "meme_directory = \"E:/BE Project/archive (3)/memes\"\n",
    "\n",
    "# Example function to classify toxicity based on image and text\n",
    "def classify_toxicity(image_path, text):\n",
    "    # Open and preprocess the image\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Prepare the inputs (image and text) with padding and truncation\n",
    "    inputs = processor(\n",
    "        text=[text], \n",
    "        images=image, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True,  # Padding the sequence to the maximum length\n",
    "        truncation=True  # Truncating if the sequence exceeds the max length\n",
    "    )\n",
    "    \n",
    "    # Get the outputs from the model\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # Extract the image and text features\n",
    "    image_features = outputs.image_embeds\n",
    "    text_features = outputs.text_embeds\n",
    "    \n",
    "    # Calculate the similarity score (cosine similarity)\n",
    "    similarity = torch.cosine_similarity(image_features, text_features)\n",
    "    \n",
    "    # Define a threshold for toxicity (example threshold)\n",
    "    toxicity_threshold = 0.4\n",
    "    \n",
    "    # If the similarity score is below the threshold, classify as toxic\n",
    "    label = \"Toxic\" if similarity < toxicity_threshold else \"Non-toxic\"\n",
    "    \n",
    "    return label, similarity.item()\n",
    "\n",
    "# Function to extract text from image using Tesseract OCR\n",
    "def extract_text_from_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    text = pytesseract.image_to_string(image)\n",
    "    return text.strip()\n",
    "\n",
    "# Process all meme images in the directory\n",
    "results = []\n",
    "for filename in os.listdir(meme_directory):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "        image_path = os.path.join(meme_directory, filename)\n",
    "        \n",
    "        # Extract text from image\n",
    "        text = extract_text_from_image(image_path)\n",
    "        \n",
    "        # If no text was extracted, skip the image\n",
    "        if not text:\n",
    "            continue\n",
    "        \n",
    "        # Classify toxicity based on image and extracted text\n",
    "        label, score = classify_toxicity(image_path, text)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"image\": filename,\n",
    "            \"extracted_text\": text,\n",
    "            \"toxicity_label\": label,\n",
    "            \"toxicity_score\": score\n",
    "        })\n",
    "\n",
    "# Convert results into a DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save results to a CSV file\n",
    "df.to_csv(\"meme_toxicity_results.csv\", index=False)\n",
    "\n",
    "# Print the DataFrame if needed\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b8e37b69-5c3a-4f19-97f7-a0830d3506e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Collecting easyocr\n",
      "  Downloading easyocr-1.7.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch in d:\\anaconda\\lib\\site-packages (from easyocr) (2.5.1)\n",
      "Requirement already satisfied: torchvision>=0.5 in d:\\anaconda\\lib\\site-packages (from easyocr) (0.20.1)\n",
      "Collecting opencv-python-headless (from easyocr)\n",
      "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: scipy in d:\\anaconda\\lib\\site-packages (from easyocr) (1.13.1)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from easyocr) (1.26.4)\n",
      "Requirement already satisfied: Pillow in d:\\anaconda\\lib\\site-packages (from easyocr) (10.3.0)\n",
      "Requirement already satisfied: scikit-image in d:\\anaconda\\lib\\site-packages (from easyocr) (0.23.2)\n",
      "Collecting python-bidi (from easyocr)\n",
      "  Downloading python_bidi-0.6.3-cp312-none-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: PyYAML in d:\\anaconda\\lib\\site-packages (from easyocr) (6.0.1)\n",
      "Collecting Shapely (from easyocr)\n",
      "  Downloading shapely-2.0.6-cp312-cp312-win_amd64.whl.metadata (7.2 kB)\n",
      "Collecting pyclipper (from easyocr)\n",
      "  Downloading pyclipper-1.3.0.post6-cp312-cp312-win_amd64.whl.metadata (9.2 kB)\n",
      "Collecting ninja (from easyocr)\n",
      "  Downloading ninja-1.11.1.3-py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from torch->easyocr) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\anaconda\\lib\\site-packages (from torch->easyocr) (4.11.0)\n",
      "Requirement already satisfied: networkx in d:\\anaconda\\lib\\site-packages (from torch->easyocr) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\lib\\site-packages (from torch->easyocr) (3.1.4)\n",
      "Requirement already satisfied: fsspec in d:\\anaconda\\lib\\site-packages (from torch->easyocr) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from torch->easyocr) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\anaconda\\lib\\site-packages (from torch->easyocr) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\anaconda\\lib\\site-packages (from sympy==1.13.1->torch->easyocr) (1.3.0)\n",
      "Requirement already satisfied: imageio>=2.33 in d:\\anaconda\\lib\\site-packages (from scikit-image->easyocr) (2.33.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in d:\\anaconda\\lib\\site-packages (from scikit-image->easyocr) (2023.4.12)\n",
      "Requirement already satisfied: packaging>=21 in d:\\anaconda\\lib\\site-packages (from scikit-image->easyocr) (23.2)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in d:\\anaconda\\lib\\site-packages (from scikit-image->easyocr) (0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda\\lib\\site-packages (from jinja2->torch->easyocr) (2.1.3)\n",
      "Downloading easyocr-1.7.2-py3-none-any.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/2.9 MB 1.7 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.2/2.9 MB 3.0 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 1.0/2.9 MB 8.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.6/2.9 MB 9.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.6/2.9 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.9/2.9 MB 11.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.9/2.9 MB 9.2 MB/s eta 0:00:00\n",
      "Downloading ninja-1.11.1.3-py3-none-win_amd64.whl (296 kB)\n",
      "   ---------------------------------------- 0.0/296.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 296.5/296.5 kB 8.9 MB/s eta 0:00:00\n",
      "Downloading opencv_python_headless-4.11.0.86-cp37-abi3-win_amd64.whl (39.4 MB)\n",
      "   ---------------------------------------- 0.0/39.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/39.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/39.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/39.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/39.4 MB 2.4 MB/s eta 0:00:17\n",
      "    --------------------------------------- 0.5/39.4 MB 2.4 MB/s eta 0:00:16\n",
      "   - -------------------------------------- 1.5/39.4 MB 5.5 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 3.2/39.4 MB 10.3 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 4.2/39.4 MB 11.7 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 5.1/39.4 MB 12.6 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 6.4/39.4 MB 14.1 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 7.7/39.4 MB 15.4 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 8.4/39.4 MB 15.7 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 9.3/39.4 MB 15.7 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 10.5/39.4 MB 21.1 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 11.8/39.4 MB 25.2 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 13.1/39.4 MB 24.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 14.0/39.4 MB 23.4 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 15.5/39.4 MB 25.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 16.6/39.4 MB 24.3 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 18.3/39.4 MB 26.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 19.3/39.4 MB 27.3 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 21.0/39.4 MB 28.5 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 22.4/39.4 MB 28.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 24.0/39.4 MB 28.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 24.6/39.4 MB 31.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 26.8/39.4 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 28.0/39.4 MB 29.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 28.8/39.4 MB 27.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 30.0/39.4 MB 28.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 30.1/39.4 MB 28.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 32.0/39.4 MB 26.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 32.8/39.4 MB 25.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 33.9/39.4 MB 23.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 35.0/39.4 MB 24.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 36.1/39.4 MB 22.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 37.4/39.4 MB 22.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.7/39.4 MB 23.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.4/39.4 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.4/39.4 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.4/39.4 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.4/39.4 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.4/39.4 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.4/39.4 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.4/39.4 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.4/39.4 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.4/39.4 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.4/39.4 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.4/39.4 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.4/39.4 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.4/39.4 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.4/39.4 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.4/39.4 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.4/39.4 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  39.4/39.4 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 39.4/39.4 MB 8.5 MB/s eta 0:00:00\n",
      "Downloading pyclipper-1.3.0.post6-cp312-cp312-win_amd64.whl (110 kB)\n",
      "   ---------------------------------------- 0.0/110.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 110.3/110.3 kB 6.3 MB/s eta 0:00:00\n",
      "Downloading python_bidi-0.6.3-cp312-none-win_amd64.whl (156 kB)\n",
      "   ---------------------------------------- 0.0/156.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 156.9/156.9 kB 9.2 MB/s eta 0:00:00\n",
      "Downloading shapely-2.0.6-cp312-cp312-win_amd64.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 0.8/1.4 MB 24.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.4/1.4 MB 18.4 MB/s eta 0:00:00\n",
      "Installing collected packages: python-bidi, pyclipper, Shapely, opencv-python-headless, ninja, easyocr\n",
      "Successfully installed Shapely-2.0.6 easyocr-1.7.2 ninja-1.11.1.3 opencv-python-headless-4.11.0.86 pyclipper-1.3.0.post6 python-bidi-0.6.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install easyocr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "965e4f63-8e18-411c-a398-459cd7e0d558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n",
      "Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete               image                                     extracted_text  \\\n",
      "0    100_M_pic_1.jpg  IComedyculluein Abba me arrest hona chahta hu,...   \n",
      "1     100_NM_pic.jpg  IG/HASTE RAHO) Girliriendbanjao DairgMikSilk g...   \n",
      "2   101_M_pic_10.jpg                           One with my two wife Day   \n",
      "3     101_NM_pic.jpg  No one Relatives when come in our house Tu peh...   \n",
      "4  102_M_pic_123.jpg          Unk futwe jinke 45 Bf hens 2 oGallu @yall   \n",
      "\n",
      "  toxicity_label  toxicity_score  \n",
      "0          Toxic        0.253802  \n",
      "1          Toxic        0.298624  \n",
      "2          Toxic        0.257361  \n",
      "3          Toxic        0.340348  \n",
      "4          Toxic        0.235905  \n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from transformers import CLIPProcessor, CLIPModel\n",
    "# from PIL import Image\n",
    "# import os\n",
    "# import pandas as pd\n",
    "import easyocr\n",
    "\n",
    "# Initialize EasyOCR Reader (for English text)\n",
    "reader = easyocr.Reader(['en'])\n",
    "\n",
    "# Initialize the CLIP model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Define the meme image directory\n",
    "meme_directory = \"E:/BE Project/archive (3)/memes\"\n",
    "\n",
    "# Example function to classify toxicity based on image and text\n",
    "def classify_toxicity(image_path, text):\n",
    "    # Open and preprocess the image\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Prepare the inputs (image and text) with padding and truncation\n",
    "    inputs = processor(\n",
    "        text=[text], \n",
    "        images=image, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True,  # Padding the sequence to the maximum length\n",
    "        truncation=True  # Truncating if the sequence exceeds the max length\n",
    "    )\n",
    "    \n",
    "    # Get the outputs from the model\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # Extract the image and text features\n",
    "    image_features = outputs.image_embeds\n",
    "    text_features = outputs.text_embeds\n",
    "    \n",
    "    # Calculate the similarity score (cosine similarity)\n",
    "    similarity = torch.cosine_similarity(image_features, text_features)\n",
    "    \n",
    "    # Define a threshold for toxicity (example threshold)\n",
    "    toxicity_threshold = 0.4\n",
    "    \n",
    "    # If the similarity score is below the threshold, classify as toxic\n",
    "    label = \"Toxic\" if similarity < toxicity_threshold else \"Non-toxic\"\n",
    "    \n",
    "    return label, similarity.item()\n",
    "\n",
    "# Function to extract text from image using EasyOCR\n",
    "def extract_text_from_image(image_path):\n",
    "    # Perform OCR using EasyOCR\n",
    "    result = reader.readtext(image_path)\n",
    "    # Combine all the text found in the image\n",
    "    text = \" \".join([entry[1] for entry in result])\n",
    "    return text.strip()\n",
    "\n",
    "# Process all meme images in the directory\n",
    "results = []\n",
    "for filename in os.listdir(meme_directory):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "        image_path = os.path.join(meme_directory, filename)\n",
    "        \n",
    "        # Extract text from image\n",
    "        text = extract_text_from_image(image_path)\n",
    "        \n",
    "        # If no text was extracted, skip the image\n",
    "        if not text:\n",
    "            continue\n",
    "        \n",
    "        # Classify toxicity based on image and extracted text\n",
    "        label, score = classify_toxicity(image_path, text)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"image\": filename,\n",
    "            \"extracted_text\": text,\n",
    "            \"toxicity_label\": label,\n",
    "            \"toxicity_score\": score\n",
    "        })\n",
    "\n",
    "# Convert results into a DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Save results to a CSV file\n",
    "df.to_csv(\"meme_toxicity_results.csv\", index=False)\n",
    "\n",
    "# Print the DataFrame if needed\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953546a8-d4fa-4d3b-9580-9538be966f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f6bfd-4fc5-472d-bd0c-54cf118c1718",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
